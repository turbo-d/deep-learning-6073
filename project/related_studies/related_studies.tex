\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Deep Learning for Automatic Downbeat Tracking\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Dale Luginbuhl}
\IEEEauthorblockA{\small\textit{Department of Computer Science} \\
\textit{University of Cincinnati}\\
Cincinnati, USA \\
luginbdr@mail.uc.edu}
\and
\IEEEauthorblockN{Atharva Pingale}
\IEEEauthorblockA{\small\textit{Department of Computer Science} \\
\textit{University of Cincinnati}\\
Cincinnati, USA \\
pingalac@mail.uc.edu}
\and
\IEEEauthorblockN{Rithvik Reddy Sama}
\IEEEauthorblockA{\small\textit{Department of Computer Science} \\
\textit{University of Cincinnati}\\
Cincinnati, USA \\
samary@mail.uc.edu}
\and
\IEEEauthorblockN{Lewis Thelen}
\IEEEauthorblockA{\small\textit{Department of Computer Science} \\
\textit{University of Cincinnati}\\
Cincinnati, USA \\
thelenlr@mail.uc.edu}
}

\maketitle

%\begin{abstract}
%This document is a model and instructions for \LaTeX.
%This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
%or Math in Paper Title or Abstract.
%\end{abstract}
%
%\begin{IEEEkeywords}
%music downbeat tracking, music information retrieval, deep learning, multimedia, review
%\end{IEEEkeywords}

\section{Introduction}
By endowing computers with the ability to listen to music we unlock a number of
applications for computer automation in music that were previously unavailable.
This field of study is referred to as Music Information Retrieval (MIR). MIR is
broken down into a number of overlapping problems as described in \cite{b1}.
Examples include:
\begin{itemize}
\item Beat and downbeat tracking
\item Tempo, time signature, and key estimation
\item Melody tracking
\item Chord detection
\item Music structure analysis
\item Musical onset/offset detection
\item Mood or emotion recognition
\end{itemize}

We explore one of these problems, downbeat tracking, in more detail. Music is
typically organized temporally around a rhythmic pulse. Each occurrence of the
pulse is referred to as a \textit{beat}. The goal of beat tracking is to annotate
the locations of these beats in an excerpt of music. Beats can further be
organized into \textit{measures}, where each measure contains the same fixed
number of beats. The first beat of each bar is typically emphasized, and this
beat is known as the \textit{downbeat}. The number of beats per measure, and
the emphasis on the downbeat give a piece of music a certain feel, and are
important qualities that shape how we interpret the music. The goal of downbeat
tracking is to annotate the location of downbeats in an excerpt of music.

Successfully annotating the location of downbeats is important for two reasons.
One, it can be used directly in applications. For example it could be used to
analyze the performance of a practicing musician and provide feedback on how
well they were able to maintain a consistent rhythmic pulse. Or it could be
used to create sync points to allow an editor to easily synchronize a
video or other multimedia application to the music. The second reason that
downbeat tracking is important, is that it can be used as a pre-processing step
in solving higher-level MIR tasks. If we have annotations of downbeat locations
in an excerpt of music, these locations provide us with a temporal framework
of the excerpt. This can simplify tasks like music structure analysis, melody
tracking, or musical onset/offset detection, because these events typically
occur on or around these beat and downbeat locations.


\section{Related Work}
\subsection{Datasets}
Various kinds of dataset are available from the various sites. All the datasets 
extracted from the various sites are in Waveform Audio File Format (.wav format). 
Ballroom with a duration of 05 hours 57 minutes and Harmonix dataset with a 
duration of 56 hours, SMC dataset has 02 hours 25 minutes, RWC-POP dataset 
has 06 hours 47 minutes, Beatles dataset has 08 hours 09 minutes, Hainsworth 
dataset has 03 hours 19 minutes, Simac dataset has 03 hoours 18 minutes, HJDB 
dataset has 03 hours 19 minutes and GTZAN dataset has 08 hours 20 minutes . 
Each dataset contains the same feature/label configuration. They consist of .wav 
audio files, each with a corresponding label file that lists the location of each
beat in seconds. An example audio file from the Ballroom dataset with only the 
downbeat labels shown is given below. Each dataset will be divided into 60\%-
20\%-20\%, where 60\% is used for the training and the 20\% is used for testing 
and rest 20\% is used for validation purpose.

\subsection{Models}
Previous models have employed a wide variety of strategies in both beat and 
downbeat tracking (we include mention of beat tracking only because beat
tracking techniques often generalize to the special case of downbeat tracking
or inadvertantly occur as an intermediary step in the process of downbeat
tracking). Earlier models \cite{b2}, \cite{b3}, \cite{b4} relied heavily on feature engineering to extract
relevant events from the music (ie chord changes, accents etc.) in order to identify
beat patterns within the audio file. More recent models have taken advantage of 
"data-driven" approaches, made possible by the rise of deep learning techniques, which allow
for the model itself to learn and then extract relevant features from the raw audio file.

Unsurprisingly, of the deep learning approaches used in downbeat tracking, sequential
models have found the best results. Earlier sequential models have used more vanilla
sequential techniques such as Long Short Term Memory (LSTM) applied directly to the raw audio data as in the case of 
\cite{b5} others have found success with more complicated approaches such as
Convolutional Recurrent Neural Networks (C-RNNs) applied to spectrogram representations
of the audio data as in the case of \cite{b6}. However, such models suffer from issues
with vanishing gradient when faced with longer music segments as is common with recurrent models
operating on time series data. More contemporary models have tried to overcome these issues by employing the then novel
Temporal Convolutional Network (TCN) architecture most notably in the case of \cite{b7}. The model employed convolutional filters across
the temporal dimension of the audio file and achieved what was until recently state of the art performance. 

Current state of the art models have taken advantage of recent advances with transformer based
architectures to achieve even higher performance at downbeat tracking as well as more general
MIR tasks such as chord change detection and music tagging than previous RNN and TCN models.
Within transformer based downbeat tracking two divergent strategies appear to have emerged. The first engages in
more explicit feature extraction as in the case of \cite{b8} where the data from individual isntruments
within the audio track are extracted and then fed together into a muli-headed attentiont transformer. The seconds
pushes further with the "data-driven" approach by relying on the model architecture to extract relevant features
from the raw audio as in the case of \cite{b9} where novel Transformer in Transformer (TNT) blocks were utilized
to mutually inform both spectral level and temporal level analysis of a raw audio file. Our current study aims to push the performance of
this so called SpecTNT by re-incorporating some degree of data engineering.


\subsection{Challenges}
Beat and downbeat tracking in music analysis pose several significant challenges. Early heuristic-based methods faced limitations in accurately detecting beats, relying on manually encoded rules that may not generalize well across diverse musical styles. Data-driven approaches, while effective, require substantial annotated data for training, which can be both costly and limited. Downbeat tracking is a challenging task because it often relies on other sub tasks such as beat tracking, tempo and time signature estimation and also because of the difficulty to state an unambiguous ground truth \cite{b8}\cite{b18}. Additionally, incorporating local spectral information and facilitating the exchange of critical local details across disparate temporal positions are formidable tasks. Both beat and downbeat tracking also grapple with the scarcity of annotated data, a prevalent issue in training data-intensive models like Transformers \cite{b8}\cite{b14}. The generalization problem of deep learning models, which may fail to adapt to unseen or novel music pieces with different time signatures, tempo ranges, or musical characteristics than the training data \cite{b8}.

\subsection{Future Work}
In the context of enhancing the performance of SOTA model for downbeat, several crucial strategies have been employed. Firstly, a pivotal step involves demixing audio sources to effectively isolate distinct instruments prior to feeding the data into the network\cite{b8}. Moreover, we seek to enhance
beat tracking by introducing instrumental attention among drum, piano, bass, vocal, and other demixed sources. This preprocessing step proves instrumental in disentangling overlapping sounds, providing the network with clearer and more discernible input. Additionally, dilated self attention models have been claimed to demonstrate powerful sequential modelling with linear complexity, potentially adaptable to more general MIR tasks\cite{b8}. Due to the limited datasets with beat and downbeat annotations, data augmentation might be required in curbing overfitting. These augmentations serve to diversify the training data, thereby enhancing the model's adaptability to a broader range of musical contexts. Some of fairly common data augmentations, each of which has an associated probability p of being applied to each training example during a training epoch. This includes the applications of highpass and lowpass filters with random cutoff frequencies (p = 0.25), random pitch shifting between -8 and 8 semitones (p = 0.5),additive white noise (p = 0.05), applying a tanh nonlinearity (p = 0.2), shifting the beat locations  forward or back by a random amount between ± 70 ms (p = 0.3), dropping a contiguous block of audio frames and beats of no more than 10\% of the input (p = 0.05), as well as a random phase inversion (p = 0.5)\cite{b13}.

% \section{Ease of Use}
% \subsection{Maintaining the Integrity of the Specifications}
% The IEEEtran class file is used to format your paper and style the text. All margins, 
% column widths, line spaces, and text fonts are prescribed; please do not 
% alter them. You may note peculiarities. For example, the head margin
% measures proportionately more than is customary. This measurement 
% and others are deliberate, using specifications that anticipate your paper 
% as one part of the entire proceedings, and not as an independent document. 
% Please do not revise any of the current designations.

% \section{Prepare Your Paper Before Styling}
% Before you begin to format your paper, first write and save the content as a 
% separate text file. Complete all content and organizational editing before 
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been 
% formatted and styled. Do not number text heads---{\LaTeX} will do that 
% for you.

% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text, 
% even after they have been defined in the abstract. Abbreviations such as 
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your 
% equations more compact, you may use the solidus (~/~), the exp function, or 
% appropriate exponents. Italicize Roman symbols for quantities and variables, 
% but not Greek symbols. Use a long dash rather than a hyphen for a minus 
% sign. Punctuate equations with commas or periods when they are part of a 
% sentence, as in:
% \begin{equation}
% a+b=\gamma\label{eq}
% \end{equation}

% Be sure that the 
% symbols in your equation have been defined before or immediately following 
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{The class file is designed for, but not limited to, six authors.} A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% %\centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
\bibitem{b1} Choi, K., Fazekas, G., Cho, K., and Sandler, M., ``A tutorial on deep learning for music information retrieval.'' arXiv preprint arXiv:1709.04396, 2017.
\bibitem{b2} Dixon, Simon (2001) Automatic Extraction of Tempo and Beat From Expressive Performances, Journal of New Music Research, 30:1, 39-58
\bibitem{b3} A. P. Klapuri, A. J. Eronen and J. T. Astola, "Analysis of the meter of acoustic musical signals," in IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, no. 1, pp. 342-355, Jan. 2006
\bibitem{b4} Goto, Masataka. (2002). An Audio-based Real-time Beat Tracking System for Music With or Without Drum-sounds. Journal of New Music Research
\bibitem{b5} S. Böck, F. Krebs and G. Widmer, "Joint beat and downbeat tracking with recurrent neural networks", Proc. Int. Soc. Music Inf. Retrieval Conf., pp. 255-261, 2016
\bibitem{b6} Tian Cheng, Satoru Fukayama, Masataka Goto, "Joint Beat and Downbeat Tracking Based on CRNN Models and a Comparison
of Using Different Context Ranges in Convolutional Layers", July 2021
\bibitem{b7} MatthewDavies, E. P., and Böck, S., ``Temporal convolutional networks for musical audio beat tracking.'' In 2019 27th European Signal Processing Conference (EUSIPCO) (pp. 1-5). IEEE, September 2019.
\bibitem{b8}  Zhao, J., Xia, G., and Wang, Y., ``Beat Transformer: Demixed beat and downbeat tracking with dilated self-attention.'' arXiv preprint arXiv:2209.07140, 2022.
\bibitem{b9} Hung, Y. N., Wang, J. C., Song, X., Lu, W. T., and Won, M., ``Modeling beats and downbeats with a time-frequency transformer.'' In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 401-405). IEEE, May 2022.
\bibitem{b10} Jia, B., Lv, J., and Liu, D., ``Deep learning-based automatic downbeat tracking: a brief review.'' Multimedia Systems, 25, 617-638, 2019.
\bibitem{b11} Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y., ``Transformer in transformer.'' Advances in Neural Information Processing Systems, 34, 15908-15919, 2021.
\bibitem{b12} Devlin, J., Chang, M. W., Lee, K., and Toutanova, K., ``Bert: Pre-training of deep bidirectional transformers for language understanding.'' arXiv preprint arXiv:1810.04805, 2018.
\bibitem{b13} Steinmetz, C. J., and Reiss, J. D., ``WaveBeat: End-to-end beat and downbeat tracking in the time domain.'' arXiv preprint arXiv:2110.01436, 2021.
\bibitem{b14} Chiu, C. Y., Ching, J., Hsiao, W. Y., Chen, Y. H., Su, A. W. Y., and Yang, Y. H., ``Source separation-based data augmentation for improved joint beat and downbeat tracking.'' In 2021 29th European Signal Processing Conference (EUSIPCO) (pp. 391-395). IEEE, August 2021. 
\bibitem{b15} Cheng, T., and Goto, M., ``U-Beat: A Multi-Scale Beat Tracking Model Based on Wave-U-Net.'' In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-5). IEEE, June 2023.
\bibitem{b16} Di Giorgi, B., Mauch, M., and Levy, M., ``Downbeat tracking with tempo-invariant convolutional neural networks.'' arXiv preprint arXiv:2102.02282, 2021.
\bibitem{b17} Hung, Y. N., Wang, J. C., Won, M., and Le, D., ``Scaling Up Music Information Retrieval Training with Semi-Supervised Learning.'' arXiv preprint arXiv:2310.01353, 2023.
\bibitem{b18} S. Durand, J. P. Bello, B. David, and G. Richard, “Robust downbeat tracking using an ensemble of convolutional networks,” IEEE/ACM Trans. Audio, Speech, and Language Process., vol. 25, no. 1, 2016. 
\end{thebibliography}
% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}